---
title: Helpful Resources
---

# {{ $frontmatter.title }}

This is a collection of resources that I find helpful for learning about kernel development.

## Reference material and documentation

## How-to guides

### By Colfax Research

I'd be remiss not to advertise the extensive blog posts we at Colfax Research have written over the years, focusing mainly on performant kernel authorship in CUTLASS.

#### GEMM and CUTLASS Basics

- [Developing CUDA Kernels for GEMM on NVIDIA Hopper Architecture using CUTLASS](https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/)
- [Tutorial: Matrix Transpose in CUTLASS](https://research.colfax-intl.com/tutorial-matrix-transpose-in-cutlass/)
- [CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA)](https://research.colfax-intl.com/tutorial-hopper-tma/)
- [CUTLASS Tutorial: Fast Matrix-Multiplication with WGMMA on NVIDIA® Hopper™ GPUs](https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/)
- [CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining](https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/)
- [Epilogue Fusion in CUTLASS with Epilogue Visitor Trees](https://research.colfax-intl.com/epilogue_visitor_tree/)
- [CUTLASS Tutorial: Persistent Kernels and Stream-K](https://research.colfax-intl.com/cutlass-tutorial-persistent-kernels-and-stream-k/)
- [CUTLASS Tutorial: Writing GEMM Kernels Using Tensor Memory For NVIDIA® Blackwell GPUs](https://research.colfax-intl.com/cutlass-tutorial-writing-gemm-kernels-using-tensor-memory-for-nvidia-blackwell-gpus/)

#### Theory of CuTe Layouts

- A note on the algebra of CuTe Layouts
- Categorical Foundations for CuTe Layouts

#### Flash Attention and Related Work

- [A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library](https://research.colfax-intl.com/nvidia-hopper-flashattention-2/)
- [Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2](https://research.colfax-intl.com/adding-fp8-to-flashattention/)
- [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://research.colfax-intl.com/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision/)
- [A User’s Guide to FlexAttention in FlashAttention CuTe DSL](https://research.colfax-intl.com/a-users-guide-to-flexattention-in-flash-attention-cute-dsl/)

## Tooling

### CUDA/C++

### CUTLASS

### CuTe DSL
